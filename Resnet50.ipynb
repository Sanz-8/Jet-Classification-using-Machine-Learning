{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO+Lt1zCypBVsi8Nrk4oweQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanz-8/Jet-Classification-using-Machine-Learning/blob/main/Resnet50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E461yhJ0njG9"
      },
      "outputs": [],
      "source": [
        "pip install -U \"datasets>=2.14.6\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten,Activation,BatchNormalization,Dropout,GlobalAveragePooling2D\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping\n",
        "from tensorflow.keras.optimizers import SGD,Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, ResNet50"
      ],
      "metadata": {
        "id": "X1Y_ytMnofI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "8TjlFGq0qoCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "token=userdata.get('HF_TOKEN')\n",
        "login(token=token)\n"
      ],
      "metadata": {
        "id": "4iYhSyTxxji1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"dl4phys/top_tagging_images\")"
      ],
      "metadata": {
        "id": "NaZDaTSOoqhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This function is to convert the image to tensor\n",
        "def preprocess_pil(pil_image,target_size=(224, 224)):\n",
        "\n",
        "    pil_image = pil_image.convert(\"RGB\") #Converting image into three channels\n",
        "\n",
        "    pil_image = pil_image.resize(target_size) #Making sure that the size of the image is compatible with ResNet50(224,224)\n",
        "\n",
        "    img_array = np.array(pil_image)\n",
        "\n",
        "    img_tensor = tf.convert_to_tensor(img_array, dtype=tf.float32) # Converting it into tensors\n",
        "\n",
        "    return img_tensor"
      ],
      "metadata": {
        "id": "VAyv4P2tpMJZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Due to limited computational power, training all the data with ResNet is not feasible. So, let's train the model with 100k, the validation and test datasets is 20k\n",
        "train_indices=np.random.choice(1200000,100000,replace=False)\n",
        "val_indices=np.random.choice(400000,20000,replace=False)\n",
        "test_indices=np.random.choice(400000,20000,replace=False)\n"
      ],
      "metadata": {
        "id": "bllIFtytpQBQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here the function performs preprocessing required by Resnet50\n",
        "def preprocess_generator(data_type,indices):\n",
        "    for (img,lbl) in zip(ds[data_type][indices]['image'],ds[data_type][indices]['label']):\n",
        "        x = preprocess_pil(img)\n",
        "        yield preprocess_input(x),lbl\n"
      ],
      "metadata": {
        "id": "jASdfK2cqbmu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The training data is streamline for efficient computation by batching the data.Also, the size of the input data is converted into (224,224,3)\n",
        "train_ds = tf.data.Dataset.from_generator(\n",
        "    lambda:preprocess_generator('train',train_indices),\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        "    )\n",
        ").cache().repeat().batch(256).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "OBqfiXpYqb8e"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Similar process is done for validation datasets\n",
        "val_ds = tf.data.Dataset.from_generator(\n",
        "    lambda:preprocess_generator('validation',val_indices),\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        "    )\n",
        ").batch(256).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "07l1K6Zdqegu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is for test datsets\n",
        "test_ds = tf.data.Dataset.from_generator(\n",
        "    lambda:preprocess_generator('test',test_indices),\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        "    )\n",
        ").batch(256).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "SB05RHGgt2NV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the ResNet50 with weights from 'imagenet'\n",
        "res_model=ResNet50(include_top=False,weights='imagenet',input_shape=(224,224,3))"
      ],
      "metadata": {
        "id": "UIevK008t_CI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a76efa53-6811-48ac-802b-abede823b380"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Freezing the layers upto 169 and allowing training the last five layers\n",
        "for l in res_model.layers[:170]:\n",
        "  l.trainable=False"
      ],
      "metadata": {
        "id": "TNgnkV66vEkJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for l in res_model.layers[160:]:\n",
        "  print(l.trainable)"
      ],
      "metadata": {
        "id": "847MY-_pvP5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fac696a-4496-45ee-f159-7c1759ac6ec6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding Dense Layer on top of the ResNet50 model\n",
        "model=Sequential()\n",
        "model.add(res_model)\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1,activation='sigmoid'))\n"
      ],
      "metadata": {
        "id": "FNcG9ZW7vTDA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Adam Optimizer with learning rate=1e-5\n",
        "model.compile(optimizer=Adam(learning_rate=1e-5),loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "wde-lCxqw5Hl"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reduce learning rate by factor of 0.5 when the validation accuracy decreases for three consecutive epochs\n",
        "reduce_alpha= ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")\n",
        "#Stop training when the validation accuracy decreases for three consecutive epochs\n",
        "stop_early=EarlyStopping(monitor='val_accuracy',patience=3,restore_best_weights=True)"
      ],
      "metadata": {
        "id": "ic61F89GxVN2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#raining the model\n",
        "hist=model.fit(train_ds, validation_data=val_ds,epochs=20,steps_per_epoch=100_000//256,callbacks=[reduce_alpha,stop_early])"
      ],
      "metadata": {
        "id": "0kwXaq8WlMGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_prob=model.predict(test_ds)"
      ],
      "metadata": {
        "id": "l6GIPPR2x3ZX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "650ba0af-52b3-459d-d3d2-5a062f570aea"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 2s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat=(y_prob>=0.5).astype(int)"
      ],
      "metadata": {
        "id": "tpqdnJUoNq-3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the accuracy of the model\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(ds['test'][test_indices]['label'],y_hat))"
      ],
      "metadata": {
        "id": "F6lzlZZ5Pj--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8111341-0180-4651-84aa-9b595754d0c3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking other metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score,roc_auc_score\n",
        "print(\"Precision Score:\",precision_score(ds['test'][test_indices]['label'],y_hat))\n",
        "print(\"Recall Score:\",recall_score(ds['test'][test_indices]['label'],y_hat))\n",
        "print(\"F1 Score:\",f1_score(ds['test'][test_indices]['label'],y_hat))\n",
        "print(\"ROC AUC Score:\",roc_auc_score(ds['test'][test_indices]['label'],y_prob))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwoo9O0Y5z4J",
        "outputId": "3d046333-50b7-41c7-de3c-5d321888060e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision Score: 0.872890245065319\n",
            "Recall Score: 0.9122072745391131\n",
            "F1 Score: 0.8921157781892604\n",
            "ROC AUC Score: 0.9574870942169041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist.history['accuracy'],'ro',linestyle='dashed')\n",
        "plt.plot(hist.history['val_accuracy'],'bo',linestyle='dashed')\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w20t_TcAq-yL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the training , validataion and test accuracy for different trainable layaers"
      ],
      "metadata": {
        "id": "dCJYNd5amhz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets(100k)\n",
        "alpha=1e-5,epochs =5,Trainable layer[165:],training_accuracy=0.8837, validation accuracy= 0.8893 and test_accuracy=0.8877  \n",
        "alpha=1e-5,epochs=5,Trainable layer[150:],training_accuracy=0.9025, validation accuracy= 0.8937 and test_accuracy=0.892  \n",
        "alpha=1e-5, epochs=9,Trainable layer[150:],training_accuracy=0.9254, validation accuracy= 0.8932 and test_accuracy=0.89576\n"
      ],
      "metadata": {
        "id": "ZorbNjFPNaYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets(200k)\n",
        "alpha=1e-6,epochs =5,Trainable layer[165:],training_accuracy=0.8837, validation accuracy= 0.8893 and test_accuracy=0.8877  "
      ],
      "metadata": {
        "id": "wVfiCfKTJfY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving the model on the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "model.save('/content/drive/MyDrive/my_models/model_res.h5')\n"
      ],
      "metadata": {
        "id": "I6TJpNJDP5r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from tensorflow.keras.models import load_model\n",
        "drive.mount('/content/drive')\n",
        "model = load_model('/content/drive/MyDrive/my_models/model_res.h5')\n"
      ],
      "metadata": {
        "id": "5A-oJ1YHOOd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fcacf41-c706-49f3-b1fe-9c1e18230f9c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IhCgwE-7OczM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}